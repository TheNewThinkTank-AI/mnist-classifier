{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MNIST-classification","text":"<p>Analyze the classic MNIST dataset using DNN, t-SNE, UMAP (VAE maybe coming up later).</p>"},{"location":"#mnist-dataset-overview","title":"MNIST Dataset Overview","text":"<p>MNIST is a dataset of 70,000 grayscale images of handwritten digits (0\u20139), each of size 28\u00d728 pixels \u2192 784 dimensions . It\u2019s often used for benchmarking machine learning algorithms, especially in visualization tasks.</p> <p>Sample images:</p>"},{"location":"#goal-of-dimensionality-reduction","title":"Goal of Dimensionality Reduction","text":"<p>We want to reduce the high-dimensional data (784D) into 2D or 3D so we can visualize it, while preserving important structure like: - Clusters of similar digits - Separation between different digit classes</p> <p>This is where t-SNE and UMAP come in.</p>"},{"location":"#t-sne-t-distributed-stochastic-neighbor-embedding","title":"t-SNE: t-Distributed Stochastic Neighbor Embedding","text":"<p>t-SNE focuses on preserving local structure \u2014 meaning it tries to keep nearby points in high-dimensional space nearby in low-dimensional space.</p> <p>\ud83d\udd0d How it works (simplified): Converts distances between points in high-dimensional space into probabilities (similar to similarities). Does the same in low-dimensional space. Minimizes the difference between these probability distributions using gradient descent . Uses a t-distribution in the low-dimensional space to avoid the \"crowding problem\". \ud83d\udcca Applied to MNIST: After applying t-SNE to MNIST, you typically get a 2D plot where: Each point represents a digit image. Points are colored by their true label (0\u20139). Similar digits cluster together.</p> <p>\u26a0\ufe0f Pros &amp; Cons:</p> PROS CONS Excellent at preserving local neighborhoods Computationally expensive Good for visualizing clusters Not good at preserving global structure Random initialization can affect results Not deterministic <p>Note: t-SNE tends to create well-separated, tight clusters but may distort the relative distances between clusters.</p>"},{"location":"#umap-uniform-manifold-approximation-and-projection","title":"UMAP: Uniform Manifold Approximation and Projection","text":"<p>UMAP also preserves local structure, but also tries to preserve some global structure, making it better for understanding overall relationships in the data.</p> <p>\ud83d\udd0d How it works (intuitively): Assumes data lies on a manifold (a curved surface embedded in high-dimensional space). Constructs a topological representation (graph) of the data. Finds a similar graph in low-dimensional space that minimizes differences. UMAP is faster than t-SNE and scales better to larger datasets.</p> <p>\ud83d\udcca Applied to MNIST: Like t-SNE, UMAP reduces MNIST to 2D/3D for visualization. Digits form clusters with clearer separation and more meaningful spacing between digit classes. Global relationships (e.g., digit 0 far from digit 1, closer to 6 or 9) are often better preserved.</p> <p>\u26a0\ufe0f Pros &amp; Cons:</p> PROS CONS Faster than t-SNE Slightly newer and less mature Preserves both local and some global structure More hyperparameters to tune Scalable to large datasets Can be harder to interpret in edge cases <p>\ud83d\udcc8 Visual Comparison on MNIST Here's what you'd typically see when plotting both methods:</p> METHOD CLUSTER SHAPE GLOBAL STRUCTURE SPEED t-SNE Compact, isolated clusters Poorly preserved Slow UMAP More spread out, connected clusters Better preserved Fast <p>For example, UMAP might show a smooth transition between 5 and 3 if they appear similar in some samples, while t-SNE might treat them as fully separate. </p>"},{"location":"#code-examples","title":"Code examples","text":"<p>See details in notebook</p> <p>Both t-SNE and UMAP are great tools, but UMAP is often preferred nowadays due to its speed and better preservation of global structure.</p>"},{"location":"DNN/dnn/","title":"Deep Neural Networks","text":"<p>Implementation details for our dense DNN model</p>"},{"location":"DNN/dnn/#why-128-neurons","title":"Why 128 Neurons?","text":"<p>For each of the hidden layers we have chosen 128 neurons. The choice of 128 here is somewhat arbitrary \u2014 it's a hyperparameter \u2014 but it follows some common practices:</p> <ul> <li>Power of 2: 128 is a power of 2 (like 32, 64, 256), which can help with memory alignment and GPU optimization.</li> <li>Balanced Complexity: It's large enough to capture meaningful patterns in the data (like edges or shapes in MNIST), but not so big that it causes overfitting or excessive computation.</li> <li>Empirical Performance: Through trial and error, people have found that 128 often works well for small datasets like MNIST.</li> <li>Historical Precedent: Many tutorials and papers use 128 as a default starting point for hidden units.</li> </ul>"},{"location":"DNN/dnn/#model-compilation","title":"Model Compilation","text":""},{"location":"DNN/dnn/#why-adam-optimizer-is-a-good-choice","title":"Why Adam optimizer is a Good Choice","text":"<p>Adam stands for Adaptive Moment Estimation , and it's one of the most popular optimization algorithms in deep learning.</p>"},{"location":"DNN/dnn/#key-advantages","title":"Key Advantages","text":"<ul> <li>Adaptive Learning Rates: Each weight gets its own learning rate that adapts during training \u2014 faster convergence.</li> <li>Combines Momentum &amp; RMSProp: Uses both momentum (to accelerate SGD) and adaptive scaling (to handle noisy gradients).</li> <li>Robust to Hyperparameters: Works well with default settings (like learning rate = 0.001), so less tuning needed.</li> <li>Good Performance on MNIST: For simple datasets like MNIST, Adam usually converges quickly and reliably.</li> </ul>"},{"location":"DNN/dnn/#how-it-works-simplified","title":"How It Works (Simplified)","text":"<p>Keeps track of moving averages of gradients (first moment) and gradient squared (second moment). Adjusts each parameter update based on these statistics. Helps avoid issues like vanishing gradients and oscillations during training. In short: Adam is fast, stable, and works well out-of-the-box, especially for small networks and datasets like MNIST.</p>"},{"location":"DNN/dnn/#sparse_categorical_crossentropy","title":"sparse_categorical_crossentropy","text":""},{"location":"DNN/dnn/#why-this-loss-function","title":"Why This Loss Function?","text":"<p>This loss is specifically designed for multi-class classification problems where:</p> <p>The labels (y_train, y_test) are integers (e.g., 0 through 9) The output layer uses softmax activation to produce a probability distribution over classes.</p>"},{"location":"DNN/dnn/#what-does-it-do","title":"What Does It Do?","text":"<p>It compares the predicted probability distribution (from softmax) with the true label (e.g., class 3), and penalizes predictions that are far from the true value.</p>"},{"location":"DNN/dnn/#example","title":"Example","text":"<p>If your model predicts:</p> <pre><code>[0.05, 0.02, 0.03, 0.8, ...]  # Predicting class 3\n</code></pre> <p>But the true label is <code>3</code></p> <p>The loss will be low because the model assigned high probability to the correct class.</p> <p>But if the model says:</p> <pre><code>[0.4, 0.3, 0.2, 0.1, ...]  # Not confident about class 3\n</code></pre> <p>Then the loss will be higher.</p>"},{"location":"DNN/dnn/#why-track-accuracy","title":"Why Track Accuracy?","text":"<p>Accuracy measures how often the model makes the correct prediction. It's easy to understand: e.g., \"97% accuracy\" means 97% of predictions were correct.</p>"},{"location":"DNN/dnn/#when-accuracy-might-be-misleading","title":"When Accuracy Might Be Misleading","text":"<p>On imbalanced datasets (e.g., 90% of samples are class 0), accuracy can be misleading. But for MNIST, classes are balanced, so accuracy is a valid and useful metric. You can also add more metrics (like precision, recall, F1-score) if you want deeper insight into performance per class.</p>"},{"location":"DNN/dnn/#model-fitting","title":"Model Fitting","text":""},{"location":"DNN/dnn/#why-use-3-epochs","title":"Why Use 3 Epochs?","text":"<p>Let's first define what an epoch is: An epoch is one full pass through the entire training dataset.</p>"},{"location":"DNN/dnn/#why-3-might-be-used","title":"Why 3 Might Be Used","text":"<ul> <li>Speed: Training for only 3 epochs is fast \u2014 useful for quick experimentation or testing code.</li> <li>Avoid Overfitting: If the dataset is very large or complex, fewer epochs may help prevent overfitting (not really the case for MNIST).</li> <li>Baseline Start: Often people start with a small number of epochs to see if the model learns at all before increasing.</li> </ul>"},{"location":"DNN/dnn/#is-3-enough","title":"Is 3 Enough?","text":"<p>For MNIST, which is a simple and clean dataset, even 1 epoch might give decent results (~90%+ accuracy). However:</p> <p>1    epoch  -&gt; ~90-92% approximate accuracy 3    epochs -&gt; ~95-97% 5-10 epochs -&gt; ~98%+</p> <p>So while 3 epochs is better than 1, it's still on the lower side for achieving the best possible performance. Usually, people train MNIST models for 5-10 epochs to reach near-optimal accuracy.</p>"},{"location":"t-SNE%20%26%20UMAP/math_background_tsne_umap/","title":"Mathematical foundations of t-SNE and UMAP","text":""},{"location":"t-SNE%20%26%20UMAP/math_background_tsne_umap/#1-t-sne-t-distributed-stochastic-neighbor-embedding","title":"1. t-SNE: t-Distributed Stochastic Neighbor Embedding","text":""},{"location":"t-SNE%20%26%20UMAP/math_background_tsne_umap/#high-level-idea","title":"High-Level Idea","text":"<p>t-SNE converts similarities between data points into probabilities and tries to minimize the difference between these probabilities in high-dimensional and low-dimensional spaces.</p>"},{"location":"t-SNE%20%26%20UMAP/math_background_tsne_umap/#step-by-step-math-breakdown","title":"Step-by-step math breakdown","text":""},{"location":"t-SNE%20%26%20UMAP/math_background_tsne_umap/#step-1-compute-pairwise-similarities-in-high-dimensional-space","title":"Step 1: Compute pairwise similarities in high-dimensional space","text":"<p>We define a conditional probability $ p_{j|i} $ that represents the similarity of point $ x_j $ to point $ x_i $. This is based on a Gaussian distribution centered at $ x_i $:</p> \\[ p_{j|i} = \\frac{\\exp\\left(-||x_i - x_j||^2 / (2\\sigma_i^2)\\right)}{\\sum_{k \\neq i} \\exp\\left(-||x_i - x_k||^2 / (2\\sigma_i^2)\\right)} \\] <ul> <li>$ \\sigma_i $ is chosen per point $ i $ so that the entropy of $ p_{j|i} $ matches a user-defined perplexity.</li> <li>Perplexity roughly corresponds to a guess of the number of neighbors around each point.</li> </ul> <p>Then, we symmetrize this into a joint probability:</p> \\[ p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2N} \\] <p>This gives us an $ N \\times N $ matrix of similarities.</p>"},{"location":"t-SNE%20%26%20UMAP/math_background_tsne_umap/#step-2-compute-similarities-in-low-dimensional-space","title":"Step 2: Compute similarities in low-dimensional space","text":"<p>In the low-dimensional embedding (say 2D), we compute a similar probability $ q_{ij} $, but now using a Student\u2019s t-distribution with 1 degree of freedom (which becomes a Cauchy distribution):</p> \\[ q_{ij} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_{k \\neq l} (1 + ||y_k - y_l||^2)^{-1}} \\] <p>This heavy-tailed distribution helps avoid the crowding problem \u2014 where many points crowd together in lower dimensions when using Gaussian distributions.</p>"},{"location":"t-SNE%20%26%20UMAP/math_background_tsne_umap/#step-3-minimize-kl-divergence","title":"Step 3: Minimize KL Divergence","text":"<p>We minimize the Kullback-Leibler divergence between the two distributions $ P $ and $ Q $:</p> \\[ KL(P || Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}} \\] <p>This cost function is minimized via gradient descent:</p> \\[ \\frac{\\partial KL}{\\partial y_i} = 4 \\sum_j (p_{ij} - q_{ij})(y_i - y_j)(1 + ||y_i - y_j||^2)^{-1} \\]"},{"location":"t-SNE%20%26%20UMAP/math_background_tsne_umap/#summary-of-t-sne-math","title":"Summary of t-SNE Math:","text":"Step Description 1 Compute Gaussian-based similarities in high-D 2 Compute t-distribution-based similarities in low-D 3 Minimize KL-divergence between the two distributions"},{"location":"t-SNE%20%26%20UMAP/math_background_tsne_umap/#2-umap-uniform-manifold-approximation-and-projection","title":"2. UMAP: Uniform Manifold Approximation and Projection","text":"<p>UMAP is inspired by topology and Riemannian geometry, assuming that data lies on a low-dimensional manifold embedded in high-dimensional space.</p> <p>It constructs a topological representation of the data and finds a low-dimensional layout that preserves this structure.</p>"},{"location":"t-SNE%20%26%20UMAP/math_background_tsne_umap/#step-1-construct-fuzzy-topological-representation","title":"Step 1: Construct Fuzzy Topological Representation","text":"<p>UMAP builds a fuzzy simplicial complex over the data:</p>"},{"location":"t-SNE%20%26%20UMAP/math_background_tsne_umap/#local-metric-estimation","title":"Local Metric Estimation:","text":"<p>For each point $ x_i $, UMAP estimates a local scale $ \\rho_i $, which is the distance to its nearest neighbor.</p> <p>Then, for each pair $ (x_i, x_j) $, it computes a fuzzy membership value:</p> \\[ w_{ij} = \\exp\\left( - \\frac{||x_i - x_j|| - \\rho_i}{\\sigma_i} \\right) \\] <ul> <li>$ \\rho_i $: distance to nearest neighbor \u2192 captures local density</li> <li>$ \\sigma_i $: scaling factor set such that the sum of weights approximates a desired local connectivity</li> </ul> <p>This results in a directed graph where edge weights represent \"fuzzy\" proximity.</p> <p>To make it undirected, UMAP combines forward and reverse edges:</p> \\[ p_{ij} = w_{ij} + w_{ji} - w_{ij} \\cdot w_{ji} \\]"},{"location":"t-SNE%20%26%20UMAP/math_background_tsne_umap/#step-2-create-low-dimensional-graph-with-similar-structure","title":"Step 2: Create Low-Dimensional Graph with Similar Structure","text":"<p>UMAP then creates a similar fuzzy simplicial complex in the low-dimensional space using a different kernel:</p> \\[ q_{ij} = (1 + a ||y_i - y_j||^{2b})^{-1} \\] <ul> <li>Parameters $ a $ and $ b $ are typically learned or set heuristically.</li> <li>The idea is to preserve the same kind of connectivity as in the high-dimensional space.</li> </ul>"},{"location":"t-SNE%20%26%20UMAP/math_background_tsne_umap/#step-3-optimize-layout-using-cross-entropy","title":"Step 3: Optimize Layout Using Cross-Entropy","text":"<p>Instead of KL-divergence like t-SNE, UMAP minimizes cross-entropy between the two fuzzy graphs:</p> \\[ CE(P, Q) = \\sum_{i \\neq j} \\left[ p_{ij} \\log \\frac{p_{ij}}{q_{ij}} + (1 - p_{ij}) \\log \\frac{1 - p_{ij}}{1 - q_{ij}} \\right] \\] <p>This loss encourages both: - Attraction between nearby points (via $ p_{ij} \\log \\frac{p_{ij}}{q_{ij}} $) - Repulsion between distant points (via $ (1 - p_{ij}) \\log \\frac{1 - p_{ij}}{1 - q_{ij}} $)</p> <p>UMAP uses stochastic gradient descent to optimize this efficiently.</p>"},{"location":"t-SNE%20%26%20UMAP/math_background_tsne_umap/#summary-of-umap-math","title":"Summary of UMAP Math:","text":"Step Description 1 Build a fuzzy graph based on local distances 2 Model low-dimensional layout with a similar graph 3 Minimize cross-entropy between graphs"},{"location":"t-SNE%20%26%20UMAP/math_background_tsne_umap/#comparison-table-t-sne-vs-umap-mathematically","title":"Comparison Table: t-SNE vs UMAP (Mathematically)","text":"Feature t-SNE UMAP Goal Preserve local neighbor relationships Preserve both local and global structure Similarity Measure Gaussian (high-D), t-distribution (low-D) Fuzzy sets with exponential decay Optimization Objective KL-divergence Cross-entropy Distance Preservation Poorly preserved globally Better preserved globally Randomness Depends on initialization Deterministic with fixed seed Speed Slower Faster due to graph-based optimization"},{"location":"t-SNE%20%26%20UMAP/math_background_tsne_umap/#practical-notes-on-mnist","title":"Practical Notes on MNIST","text":"<p>When applied to MNIST: - t-SNE tends to produce compact clusters with clear separation between digit classes, but may not show how clusters relate to each other. - UMAP often shows more meaningful spacing between clusters (e.g., 0 near 6/9, 1 far from 8), and sometimes reveals substructures within clusters (e.g., variations in how people write digits).</p>"},{"location":"t-SNE%20%26%20UMAP/math_background_tsne_umap/#coming-up","title":"Coming up","text":"<ul> <li>For t-SNE: perplexity, early exaggeration, and gradient descent steps.</li> <li>For UMAP: The role of Riemannian manifolds, fuzzy topological structures, and spectral graph theory.</li> </ul>"}]}